from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import time
import json
import os
import psutil
import torch
import threading


@dataclass
class GPUSnapshot:
    """Single time-point GPU metrics for one device"""
    timestamp: float  # time.time() for precise timing
    gpu_id: int
    memory_allocated_mb: float
    memory_reserved_mb: float  # torch.cuda.memory_reserved()
    utilization_percent: float  # from pynvml
    temperature_celsius: Optional[int] = None  # from pynvml (optional)


@dataclass
class GPUTimeSeries:
    """Collection of time-series snapshots for a GPU during an operation"""
    gpu_id: int
    snapshots: List[GPUSnapshot]
    peak_memory_mb: float
    peak_utilization_percent: float
    avg_utilization_percent: float

    def to_dict(self) -> Dict:
        return {
            'gpu_id': self.gpu_id,
            'peak_memory_mb': round(self.peak_memory_mb, 2),
            'peak_utilization_percent': round(self.peak_utilization_percent, 2),
            'avg_utilization_percent': round(self.avg_utilization_percent, 2),
            'sample_count': len(self.snapshots),
            'snapshots': [
                {
                    'timestamp': snap.timestamp,
                    'memory_allocated_mb': round(snap.memory_allocated_mb, 2),
                    'memory_reserved_mb': round(snap.memory_reserved_mb, 2),
                    'utilization_percent': round(snap.utilization_percent, 2),
                    'temperature_celsius': snap.temperature_celsius
                } for snap in self.snapshots
            ]
        }


@dataclass
class ComputationalMetrics:
    operation: str
    wall_time: float
    cpu_percent: float
    memory_mb: float
    gpu_memory_mb: float  # DEPRECATED: Keep for backward compatibility, represents GPU 0 delta
    tokens_processed: int
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    # NEW FIELDS:
    gpu_time_series: Optional[Dict[int, GPUTimeSeries]] = None  # key: gpu_id, value: GPUTimeSeries
    gpu_count: int = 0

    def to_dict(self) -> Dict:
        base_dict = {
            'operation': self.operation,
            'wall_time_seconds': round(self.wall_time, 4),
            'cpu_percent': round(self.cpu_percent, 2),
            'memory_mb': round(self.memory_mb, 2),
            'gpu_memory_mb': round(self.gpu_memory_mb, 2),  # Backward compatibility
            'tokens_processed': self.tokens_processed,
            'timestamp': self.timestamp,
            'gpu_count': self.gpu_count
        }

        # Add per-GPU metrics if available
        if self.gpu_time_series:
            base_dict['gpu_metrics'] = {
                str(gpu_id): ts.to_dict()
                for gpu_id, ts in self.gpu_time_series.items()
            }

        return base_dict


class PerfomanceMonitor:
    def __init__(self, enable_gpu_monitoring: bool = True, sampling_interval: float = 0.1):
        """
        Args:
            enable_gpu_monitoring: If True, enables detailed GPU monitoring with pynvml
            sampling_interval: Time in seconds between GPU samples (default: 0.1s)
        """
        self.metrics: List[ComputationalMetrics] = []
        self.process = psutil.Process()
        self.current_operation: Optional[str] = None
        self.start_time: Optional[float] = None
        self.start_memory: Optional[float] = None
        self.start_gpu_memory: Optional[float] = None

        # NEW: GPU monitoring configuration
        self.enable_gpu_monitoring = enable_gpu_monitoring
        self.sampling_interval = sampling_interval
        self.gpu_count = 0
        self.nvml_initialized = False
        self.gpu_handles = []  # List of pynvml device handles

        # NEW: Threading for background sampling
        self._sampling_thread: Optional[threading.Thread] = None
        self._stop_sampling = threading.Event()
        self._gpu_snapshots: Dict[int, List[GPUSnapshot]] = {}  # key: gpu_id
        self._sampling_lock = threading.Lock()

        # Initialize NVML if GPU monitoring enabled
        if self.enable_gpu_monitoring and torch.cuda.is_available():
            self._initialize_nvml()

    def _initialize_nvml(self):
        """Initialize NVIDIA Management Library"""
        try:
            import pynvml
            pynvml.nvmlInit()
            self.nvml_initialized = True
            self.gpu_count = torch.cuda.device_count()

            # Get handles for all GPUs
            for i in range(self.gpu_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                self.gpu_handles.append(handle)

        except Exception as e:
            print(f"Warning: Failed to initialize NVML: {e}")
            self.nvml_initialized = False
            self.enable_gpu_monitoring = False

    def _sample_gpu_metrics(self):
        """Background thread function to sample GPU metrics"""
        import pynvml

        while not self._stop_sampling.is_set():
            current_time = time.time()

            for gpu_id in range(self.gpu_count):
                try:
                    # Get memory from torch
                    memory_allocated = torch.cuda.memory_allocated(gpu_id) / (1024 ** 2)
                    memory_reserved = torch.cuda.memory_reserved(gpu_id) / (1024 ** 2)

                    # Get utilization from pynvml
                    handle = self.gpu_handles[gpu_id]
                    util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)

                    snapshot = GPUSnapshot(
                        timestamp=current_time,
                        gpu_id=gpu_id,
                        memory_allocated_mb=memory_allocated,
                        memory_reserved_mb=memory_reserved,
                        utilization_percent=util.gpu,
                        temperature_celsius=temp
                    )

                    with self._sampling_lock:
                        if gpu_id not in self._gpu_snapshots:
                            self._gpu_snapshots[gpu_id] = []
                        self._gpu_snapshots[gpu_id].append(snapshot)

                except Exception as e:
                    # Log error but continue sampling
                    pass

            # Sleep for sampling interval
            self._stop_sampling.wait(self.sampling_interval)

    def _start_background_sampling(self):
        """Start background GPU sampling thread"""
        if not self.enable_gpu_monitoring or not self.nvml_initialized:
            return

        with self._sampling_lock:
            self._gpu_snapshots = {}

        self._stop_sampling.clear()
        self._sampling_thread = threading.Thread(
            target=self._sample_gpu_metrics,
            daemon=True,
            name="GPUMonitorThread"
        )
        self._sampling_thread.start()

    def _stop_background_sampling(self) -> Dict[int, GPUTimeSeries]:
        """Stop background sampling and compute aggregated metrics"""
        if not self.enable_gpu_monitoring or not self.nvml_initialized:
            return {}

        # Signal thread to stop
        self._stop_sampling.set()

        # Wait for thread to finish (with timeout)
        if self._sampling_thread and self._sampling_thread.is_alive():
            self._sampling_thread.join(timeout=1.0)

        # Process collected snapshots
        gpu_time_series = {}

        with self._sampling_lock:
            for gpu_id, snapshots in self._gpu_snapshots.items():
                if not snapshots:
                    continue

                # Calculate peak and average metrics
                peak_memory = max(s.memory_allocated_mb for s in snapshots)
                peak_util = max(s.utilization_percent for s in snapshots)
                avg_util = sum(s.utilization_percent for s in snapshots) / len(snapshots)

                gpu_time_series[gpu_id] = GPUTimeSeries(
                    gpu_id=gpu_id,
                    snapshots=snapshots,
                    peak_memory_mb=peak_memory,
                    peak_utilization_percent=peak_util,
                    avg_utilization_percent=avg_util
                )

        return gpu_time_series

    def start_operation(self, operation_name: str):
        """Start monitoring an operation with GPU sampling"""
        self.current_operation = operation_name
        self.start_time = time.time()
        self.start_memory = self.process.memory_info().rss / (1024 ** 2)  # Bytes para mb

        # Backward compatible: track GPU 0 memory delta
        if torch.cuda.is_available():
            self.start_gpu_memory = torch.cuda.memory_allocated(0) / (1024 ** 2)
        else:
            self.start_gpu_memory = 0.0

        # NEW: Start background GPU sampling
        self._start_background_sampling()

    def end_operation(self, tokens: int = 0) -> Tuple[ComputationalMetrics, Dict]:
        """End operation monitoring and collect all metrics"""
        if self.current_operation is None:
            raise RuntimeError("Call start_operation() first")

        # Stop background sampling FIRST to get complete data
        gpu_time_series = self._stop_background_sampling()

        # Calculate wall time and CPU/memory metrics
        elapsed_time = time.time() - self.start_time
        end_memory = self.process.memory_info().rss / (1024 ** 2)
        memory_used = end_memory - self.start_memory

        # Backward compatible: GPU 0 memory delta
        if torch.cuda.is_available():
            end_gpu_memory = torch.cuda.memory_allocated(0) / (1024 ** 2)
            gpu_memory_used = end_gpu_memory - self.start_gpu_memory
        else:
            gpu_memory_used = 0.0

        cpu_percent = self.process.cpu_percent(interval=0.5)

        # Create metrics with new GPU time-series data
        metric = ComputationalMetrics(
            operation=self.current_operation,
            wall_time=elapsed_time,
            cpu_percent=cpu_percent,
            memory_mb=memory_used,
            gpu_memory_mb=gpu_memory_used,  # Backward compatible
            tokens_processed=tokens,
            gpu_time_series=gpu_time_series if gpu_time_series else None,
            gpu_count=self.gpu_count
        )

        self.metrics.append(metric)

        # Reset state
        self.current_operation = None
        self.start_time = None
        self.start_memory = None
        self.start_gpu_memory = None

        return metric, metric.to_dict()

    def get_summary(self) -> Dict:
        """Generate summary statistics including per-GPU metrics"""
        if not self.metrics:
            return {"error": "No metrics"}

        operations = {}
        for metric in self.metrics:
            op = metric.operation
            if op not in operations:
                operations[op] = {
                    'count': 0,
                    'total_time': 0.0,
                    'total_memory': 0.0,
                    'total_gpu_memory': 0.0,
                    'total_tokens': 0,
                    'gpu_metrics': {}  # NEW: per-GPU aggregation
                }

            operations[op]['count'] += 1
            operations[op]['total_time'] += metric.wall_time
            operations[op]['total_memory'] += metric.memory_mb
            operations[op]['total_gpu_memory'] += metric.gpu_memory_mb
            operations[op]['total_tokens'] += metric.tokens_processed

            # NEW: Aggregate GPU time-series data
            if metric.gpu_time_series:
                for gpu_id, ts in metric.gpu_time_series.items():
                    if gpu_id not in operations[op]['gpu_metrics']:
                        operations[op]['gpu_metrics'][gpu_id] = {
                            'total_peak_memory': 0.0,
                            'total_peak_util': 0.0,
                            'total_avg_util': 0.0,
                            'count': 0
                        }
                    gm = operations[op]['gpu_metrics'][gpu_id]
                    gm['total_peak_memory'] += ts.peak_memory_mb
                    gm['total_peak_util'] += ts.peak_utilization_percent
                    gm['total_avg_util'] += ts.avg_utilization_percent
                    gm['count'] += 1

        summary = {}
        for op, data in operations.items():
            op_summary = {
                'count': data['count'],
                'avg_time_seconds': round(data['total_time'] / data['count'], 4),
                'total_time_seconds': round(data['total_time'], 4),
                'avg_memory_mb': round(data['total_memory'] / data['count'], 2),
                'avg_gpu_memory_mb': round(data['total_gpu_memory'] / data['count'], 2),
                'total_tokens': data['total_tokens']
            }

            # NEW: Add per-GPU summary
            if data['gpu_metrics']:
                op_summary['gpu_summary'] = {}
                for gpu_id, gm in data['gpu_metrics'].items():
                    op_summary['gpu_summary'][f'gpu_{gpu_id}'] = {
                        'avg_peak_memory_mb': round(gm['total_peak_memory'] / gm['count'], 2),
                        'avg_peak_utilization': round(gm['total_peak_util'] / gm['count'], 2),
                        'avg_utilization': round(gm['total_avg_util'] / gm['count'], 2)
                    }

            summary[op] = op_summary

        return summary

    def save_metrics(self, filepath: str):
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        output = {
            'summary': self.get_summary(),
            'detailed_metrics': [m.to_dict() for m in self.metrics],
            'total_operations': len(self.metrics),
            'recorded_at': datetime.now().isoformat()
        }
        with open(filepath, 'w') as f:
            json.dump(output, f, indent=4)

    def __del__(self):
        """Cleanup NVML resources"""
        if self.nvml_initialized:
            try:
                import pynvml
                pynvml.nvmlShutdown()
            except:
                pass
