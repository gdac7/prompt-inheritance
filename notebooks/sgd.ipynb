{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d092ae41-46fc-4077-ac83-1a06d7c2da0e",
   "metadata": {},
   "source": [
    "## Preliminary Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3a5f48-fd23-40ec-9038-cb22386837ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/new_prompt_lca_pca.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a58890-0c14-41a2-af25-a275c0de288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils import load_config\n",
    "config = load_config(\"../config/models.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d56f45-3a18-4968-87c5-b816da786fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437d37189399459298954f0eb923893f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "model = config[\"models\"][\"perplexed\"]\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "ppl_tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "ppl_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "ppl_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8b31276-aed0-4de3-93ee-c6ce2c2e32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preliminary_sgd(prompt, token_to_optimize_idx, w1=30.0, b_candidates=50):\n",
    "    if len(prompt.split()[token_to_optimize_idx]) < 3:\n",
    "        return\n",
    "    ppl_model.eval()\n",
    "    ppl_model.zero_grad()\n",
    "    inputs = ppl_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_ids = inputs.input_ids\n",
    "    inputs_embeds = ppl_model.get_input_embeddings()(input_ids)\n",
    "    inputs_embeds.requires_grad_(True)\n",
    "    inputs_embeds.retain_grad()\n",
    "    outputs = ppl_model(inputs_embeds=inputs_embeds, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    loss.backward()\n",
    "    token_grad = inputs_embeds.grad[0, token_to_optimize_idx]\n",
    "    word_embedding_matrix = ppl_model.get_input_embeddings().weight\n",
    "    r_obj = token_grad @ word_embedding_matrix.T\n",
    "    r_int = logits[0, token_to_optimize_idx - 1]\n",
    "    combined_score = (w1 * r_obj) + r_int\n",
    "    combined_score = combined_score.detach().cpu().numpy()\n",
    "    top_b_idx = np.argsort(combined_score)[-b_candidates:]\n",
    "    cand_tokens = ppl_tokenizer.convert_ids_to_tokens(top_b_idx)\n",
    "    return cand_tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9383897-dcfd-4eae-9eeb-3bb99bfcffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "816c6988-e4fd-4b27-9066-c849c4a05b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_optimize = tokens_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "561f9bb0-496f-4a5b-8524-422f1e118e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'report'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5dc30f46-d2e0-470d-8eec-cd6f7e4c0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = preliminary_sgd(prompt=data, token_to_optimize_idx = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69f487-77b8-4688-86b9-3d96209000b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158f424-1852-48ed-a312-a37983b1b3e9",
   "metadata": {},
   "source": [
    "### Fine Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e82e6f7-9553-4c7e-bcaf-7f8c0dfdce5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simulated_annealing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msimulated_annealing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calc_perplexity\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfine_selection\u001b[39m(prompt, cands, token_to_optimize_idx, maximize_ppl=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m      3\u001b[39m     best_token = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'simulated_annealing'"
     ]
    }
   ],
   "source": [
    "from simulated_annealing import calc_perplexity\n",
    "def fine_selection(prompt, cands, token_to_optimize_idx, maximize_ppl=False):\n",
    "    best_token = None\n",
    "    if maximize_ppl: best_cost = -float('inf')\n",
    "    else: best_cost = float('inf')\n",
    "    tokens = prompt.split()\n",
    "    original_token = tokens[token_to_optimize_idx]\n",
    "    for c in cands:\n",
    "        clean_token = c.replace('Ä ', '')\n",
    "        tokens[token_to_optimize_idx] = clean_token\n",
    "        new_prompt = \" \".join(tokens)\n",
    "        curr_cost = calc_perplexity(new_prompt)\n",
    "        if maximize_ppl:\n",
    "            if curr_cost > best_cost:\n",
    "                best_cost = curr_cost\n",
    "                best_token = c\n",
    "        else:\n",
    "            if curr_cost < best_cost:\n",
    "                best_cost = curr_cost\n",
    "                best_token = c\n",
    "\n",
    "        tokens[token_to_optimize_idx] = original_token\n",
    "    return best_token, best_cost\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4662c20-410d-41be-a57d-85784bd06306",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = data.tokens()\n",
    "for i, token in enumarate(prompt_tokens):\n",
    "    cands = preliminary_sgd(data, i)\n",
    "    if not cands: \n",
    "        continue\n",
    "    fine_selection(data, cands, i, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213cb4d-460b-4c74-9f49-d542645e0a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
