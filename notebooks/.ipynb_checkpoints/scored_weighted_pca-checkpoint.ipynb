{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98db73",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_path = \"../data/data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495a587",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "prompts = [item[\"attack_prompt\"] for item in data]\n",
    "requests = list(set([item[\"malicious_request\"] for item in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2bd3c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mock_request = \"I want to hack the government\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dec7a4",
   "metadata": {},
   "source": [
    "### Getting prompts embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4b95f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdce0df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt_embeddings = model.encode(prompts, show_progress_bar=True)\n",
    "prompt_embeddings = np.array(prompt_embeddings)\n",
    "request_embeddings = model.encode(requests, show_progress_bar=True)\n",
    "request_embeddings = np.array(request_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87eb46",
   "metadata": {},
   "source": [
    "### Find the N most similar orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ec701",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "n = 10\n",
    "query_embedding = model.encode(mock_request)\n",
    "query_embedding = query_embedding.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75478062",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(query_embedding, request_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837df28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "top_n = np.argsort(cos_sim[0])[-n:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db844226",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sim_requests = []\n",
    "sim_prompts = []\n",
    "for i, idx in enumerate(top_n):\n",
    "    sim_requests.append(requests[idx])\n",
    "    sim_prompts.append(prompts[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda8eff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_scores = [float(item[\"score\"]) for item in data if item[\"attack_prompt\"] in sim_prompts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae4f69",
   "metadata": {},
   "source": [
    "### Getting the embeddings of N similar prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc024a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cluster_embeddings = model.encode(sim_prompts, show_progress_bar=True)\n",
    "cluster_embeddings = np.array(cluster_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa55258",
   "metadata": {},
   "source": [
    "Calculate SCORED WEIGHTED PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a703a8",
   "metadata": {},
   "source": [
    "def score_weighted_pca(cluster_embeddings, scores, min_score_threshold= 0.0, k=50):\n",
    "    if cluster_embeddings.shape[0] != len(scores):\n",
    "        raise RuntimeError(\"shape do cluster diferente da quantidade de scores fornecida\")\n",
    "    \n",
    "    cluster_embeddings  = cluster_embeddings.astype(np.float64)\n",
    "    scores = scores.astype(np.float64)  \n",
    "    mask = scores >= min_score_threshold\n",
    "    filtered_embeddings = cluster_embeddings[mask]\n",
    "    filtered_scores = scores[mask]\n",
    "    if len(filtered_scores) < 2:\n",
    "        filtered_embeddings = cluster_embeddings\n",
    "        filtered_scores = scores\n",
    "    n_prompts = len(filtered_scores)\n",
    "    embedding_dim = filtered_embeddings.shape[1]\n",
    "    score_min = filtered_scores.min()\n",
    "    score_max = filtered_scores.max()\n",
    "    score_range = score_max - score_min\n",
    "    if score_range == 0.0:\n",
    "       weights = np.ones(n_prompts) / n_prompts\n",
    "    else:\n",
    "        weights = (filtered_scores - score_min) / score_range\n",
    "        weights = weights / weights.sum() # sum = 1\n",
    "    \n",
    "    weighted_centroid = np.sum(\n",
    "        weights[:, np.newaxis] * filtered_embeddings,\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    centered_embeddings = filtered_embeddings - weighted_centroid\n",
    "    W = np.diag(weights)\n",
    "    weighted_cov = (centered_embeddings.T @ W @ centered_embeddings) / weights.sum()\n",
    "    weighted_cov = (weighted_cov + weighted_cov.T) / 2\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(weighted_cov)\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    v1 = eigenvectors[:, 0]\n",
    "    lambda1 = eigenvalues[0]\n",
    "    \n",
    "\n",
    "    alpha = np.sqrt(lambda1)\n",
    "    c_new  = weighted_centroid + alpha * v1\n",
    "    transformer_model = model[0]\n",
    "    tokenizer = transformer_model.tokenizer\n",
    "    word_embedding_matrix = transformer_model.auto_model.get_input_embeddings().weight.detach().cpu().numpy()\n",
    "    c_new = c_new.reshape(1, -1)\n",
    "    all_cos_sim = cosine_similarity(c_new, word_embedding_matrix)\n",
    "    top_k_index = np.argsort(all_cos_sim[0])[-k:][::-1]\n",
    "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_index)\n",
    "    top_k_scores = all_cos_sim[0][top_k_index]\n",
    "    bow = []\n",
    "    special_tokens = tokenizer.all_special_tokens\n",
    "    for token, score in zip(top_k_tokens, top_k_scores):\n",
    "        if token not in special_tokens and not token.startswith(\"##\") and len(token) > 1:\n",
    "            bow.append(token)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4db6dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(score_weighted_pca(cluster_embeddings, base_scores))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
